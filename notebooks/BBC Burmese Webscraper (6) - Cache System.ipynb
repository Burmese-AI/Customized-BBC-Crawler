{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73c364e-aa87-4e3f-a03b-5ede253d83e1",
   "metadata": {},
   "source": [
    "# BBC Burmese Webscraper (6): Adding Redis Cache System \n",
    "# to Scraping Headers, Dates, and Contents \n",
    "# from All Pages of All Topics\n",
    "by <a href=\"https://www.linkedin.com/in/la-wun-nannda-b047681b5/\">`La Wun Nannda`</a>\n",
    "\n",
    "## Changes\n",
    "- The previous notebook can scrape the entire website, i.e., all topics and all pages in each topic. This is done via extending the scraping process of one topic to all topics. But it does not consider about possible connection loss.\n",
    "- Here, I use `redis` to store the current page url.\n",
    "- If the connection is lost, the stored url can be retrieved to resume scraping once the connection comes back.\n",
    "- To achieve that, the while loop is utilized.\n",
    "\n",
    "## References\n",
    "Here are documentations and tutorials I referenced:\n",
    "- https://youtu.be/reNPNDustQU?si=ziPUyuZ5iW68y7LA\n",
    "- https://redis.io/docs/latest/operate/oss_and_stack/install/install-redis/install-redis-on-windows/\n",
    "- https://redis.io/docs/latest/develop/connect/clients/python/redis-py/\n",
    "- https://fastapi.tiangolo.com/learn/\n",
    "- https://www.uvicorn.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c7a315-c1ab-4ec0-8a6d-175d101e7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main website url\n",
    "main_url = \"https://www.bbc.com/burmese\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4bbbfa-9e6a-4259-a137-6b5671e21cd4",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fceff4c2-e6d2-4b51-868b-1c1e50f5f490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup # this module helps in web scrapping\n",
    "import requests  # this module helps us to download a web page\n",
    "import pandas as pd\n",
    "import redis # this module tracks the scraping progress\n",
    "import time # this module pauses the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419a482-316c-4ea1-b2bd-40886420ddfb",
   "metadata": {},
   "source": [
    "## Initialization For Cache System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1054a7e5-37e0-46d7-b898-6014f8b87f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redis\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "SCRAPING_KEY = \"currently_scraping_url\" # the directory name\n",
    "error_found = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1b4d8-f959-408c-8af9-6719dc24419f",
   "metadata": {},
   "source": [
    "## Necessary Functions to Use Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed4feb56-c868-49b6-874d-d8984ca1d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to start and track the scraping process\n",
    "def setCache(url):\n",
    "    r.set(SCRAPING_KEY, url) # store url in cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af90af0-b93d-487c-8edb-1ebf27e93c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reset scraping\n",
    "def resetCache():\n",
    "    r.set(SCRAPING_KEY, main_url) # set cache to main url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b87322f-f936-479b-a4c5-7afc3ee41056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to resume scraping\n",
    "def scrapeCache():\n",
    "    cache_url = r.get(SCRAPING_KEY).decode('utf-8') # retrieve current url\n",
    "    global error_found # refer to global variable\n",
    "    try:\n",
    "        response = requests.get(cache_url) # try scraping\n",
    "        error_found = False\n",
    "        return response\n",
    "    except:\n",
    "        error_found = True\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76c31c-765b-461c-8c43-32b46a01a978",
   "metadata": {},
   "source": [
    "## Necessary Functions to Scrape One Page of One Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c42acc-1d72-42f7-979e-2b28994c3c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get soup with URL input of one topic\n",
    "def webScraper(url):\n",
    "    setCache(url) # store url in cache\n",
    "    response = scrapeCache() # try scraping the url from cache\n",
    "    while(error_found): # continue scraping until the connection comes back\n",
    "        print(f\"  - Connection Error. Retrying in 5 seconds...\")\n",
    "        time.sleep(5) # wait 5 seconds\n",
    "        response = scrapeCache()\n",
    "    soup = BeautifulSoup(response.content, 'html5lib')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6fe67f4-57c5-407e-a394-619dba457c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get specific elements within a soup of one topic\n",
    "def soupParser(soup):\n",
    "    news_headers_soup = soup.find_all(\"a\", {\"class\":\"focusIndicatorDisplayBlock\"}) # filter headers\n",
    "    datetime_soup = soup.find_all(\"time\", {\"class\":\"promo-timestamp\"}) # filter datetime\n",
    "    return news_headers_soup, datetime_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb8e90e0-c110-4f36-b947-96c89c44deaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract Burmese content from a content url of one topic\n",
    "def contentScraper(content_url, soup):\n",
    "    burmese_content = \"\"\n",
    "    alphabets = ['a', 'b', 'c', 'd', 'e',\n",
    "                'f', 'g', 'h', 'i', 'j',\n",
    "                'k', 'l', 'm', 'n', 'o',\n",
    "                'p', 'q', 'r', 's', 't',\n",
    "                'u', 'v', 'w', 'x', 'y',\n",
    "                'z']\n",
    "    \n",
    "    symbols = [\"\\'\", \"(\", \")\"]\n",
    "    \n",
    "    for p_element in soup.find_all(\"p\"):\n",
    "        try: # None Type can cause error\n",
    "            content = p_element.string.strip()\n",
    "            for char in content:\n",
    "                if (char.lower() in alphabets) or (char in symbols): # do not add non-Burmese characters or symbols\n",
    "                    continue\n",
    "                burmese_content += char # add Burmese characters only\n",
    "        except:\n",
    "            pass\n",
    "    return burmese_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66fa3078-db10-473d-b8a7-3c9112ada2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create lists for a page of one topic\n",
    "'''Return three lists containing corresponding entries of a page of one topic'''\n",
    "def appendListPerPage(news_headers_soup, datetime_soup):\n",
    "    news_headers_per_page = []\n",
    "    datetime_per_page = []\n",
    "    contents_per_page = []\n",
    "    \n",
    "    if len(news_headers_soup) == len(datetime_soup): # each header should have a corresponding date\n",
    "        \n",
    "        for i in range(len(news_headers_soup)): # get index of headers for one page\n",
    "    \n",
    "            # list 1 for multiple headers in a page\n",
    "            try: # for news headers without video tag # video tagged ones will cause errors\n",
    "                news_headers_per_page.append(news_headers_soup[i].string.strip()) # convert 'BeautifulSoup string' to 'Python string' # add content to list 1\n",
    "            except AttributeError: # # for news headers with video tag\n",
    "                '''list() is used to convert 'BeautifulSoup tag' object to 'list' to enable iteration'''\n",
    "                news_headers_per_page.append(list(news_headers_soup[i].span)[1].strip()) # convert 'BeautifulSoup string' to 'Python string' # add content to list 1\n",
    "    \n",
    "            # list 2 for date and time in a page\n",
    "            datetime_per_page.append(datetime_soup[i].string.strip()) # convert 'BeautifulSoup string' to 'Python string' # add content to list 2\n",
    "\n",
    "            # list 3 for contents of all headers in a page (contents of multiple headers)\n",
    "            content_url = news_headers_soup[i].attrs['href'] # get a link from 'n' element\n",
    "            content_soup = webScraper(content_url) # pass the link to create a new soup\n",
    "            content_per_header = contentScraper(content_url, content_soup) # this new soup is used for content scraping\n",
    "            contents_per_page.append(content_per_header)\n",
    "        \n",
    "        if (len(news_headers_per_page) == len(news_headers_soup)) & (len(datetime_per_page) == len(datetime_soup)) & (len(contents_per_page) != 0): # if everything is added to two lists\n",
    "            return news_headers_per_page, datetime_per_page, contents_per_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936fff06-bb6d-4674-a0a9-3e9a899c4495",
   "metadata": {},
   "source": [
    "## Necessary Functions to Scrape All Pages of One Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da1e6acf-8a0d-4cad-ae77-e370388b6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get next page url of one topic\n",
    "def getNextPageUrl(web_url, soup):\n",
    "    next_page_soup = soup.find(\"a\", {\"aria-labelledby\":\"pagination-next-page\", \"class\":\"focusIndicatorOutlineBlack\", \"href\":True})\n",
    "    complete_url = web_url + next_page_soup.attrs['href']\n",
    "    return complete_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba09d7a1-a1fd-48c8-9bb9-d945bae0909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the last page index of one topic\n",
    "def getPageLimit(soup):\n",
    "    last_page_soup = soup.find_all(\"a\", {\"class\":\"focusIndicatorOutlineBlack\", \"href\":True})[-2] # filter the second last item # discard last item\n",
    "    last_page_index = int(last_page_soup.string)\n",
    "    return last_page_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805d2160-264a-4622-9f36-69df0d1961b5",
   "metadata": {},
   "source": [
    "## Necessary Function to Scrape All Pages of All Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc5319d9-2b29-4194-bcc9-acb4a4a7f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get all topics urls\n",
    "def getTopicsUrls(soup):\n",
    "    default_page_initial = \"https://www.bbc.com\"\n",
    "    list_of_topics_urls = []\n",
    "    \n",
    "    list_of_topics = soup.find_all(\"a\", {\"class\":\"focusIndicatorRemove bbc-qh9e61 e11sm0on3\" , \"href\":True}) # filter topics\n",
    "    for topic in list_of_topics:\n",
    "        list_of_topics_urls.append(default_page_initial + topic.attrs['href'])\n",
    "    \n",
    "    return list_of_topics_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccea35b0-9e98-4c03-8636-54b4388ebd56",
   "metadata": {},
   "source": [
    "## Necessary Functions to Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d113b6b-4c33-46d3-8fde-062c23938d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create an empty dataframe\n",
    "def createDF():\n",
    "    BBC = {}\n",
    "    BBC['News Header'] = []\n",
    "    BBC['Time'] = []\n",
    "    BBC['Content'] = []\n",
    "    df = pd.DataFrame(BBC)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5d860fa-7c0f-4bec-be41-9ff5b3c62167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to add a row to an existing dataframe\n",
    "def addDF(existing_df, row_list):\n",
    "    next_index = len(existing_df.index)\n",
    "    existing_df.loc[next_index] = row_list\n",
    "    return existing_df\n",
    "\n",
    "# function to add rows (of one full page) to an existing dataframe\n",
    "def addDFs(existing_df, news_headers_list, datetime_list, contents_list):\n",
    "    for i in range(len(news_headers_list)):\n",
    "        existing_df = addDF(existing_df, [news_headers_list[i], datetime_list[i], contents_list[i]]) # addDF(df, [item1, item2, item3])\n",
    "    return existing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bea827f-44d8-4cc3-954c-3d256a6ed3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to produce a spreadsheet\n",
    "def exportExcel(df, file_name_string: str):\n",
    "    parent_directory = '../spreadsheets/'\n",
    "    extension = '.xlsx' \n",
    "    full_directory = parent_directory + file_name_string + extension\n",
    "\n",
    "    try:\n",
    "        df.to_excel(full_directory, index=False)\n",
    "        return True\n",
    "    except:\n",
    "        print(\"Error creating a spreadsheet!\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719819a5-c1ce-460e-8e25-cd6aec0091bd",
   "metadata": {},
   "source": [
    "## Functions Integration to Scrape All Pages of One Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b94b09bf-39c8-4a7d-b3aa-bcfb26cc0d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape all pages of one topic\n",
    "def scrapeTopic(web_url, file_name_string: str):\n",
    "    news_headers_per_page = []\n",
    "    datetime_per_page = []\n",
    "    contents_per_page = []\n",
    "\n",
    "    # initialize an empty dataframe for each topic\n",
    "    df = createDF()\n",
    "    \n",
    "    # the first page\n",
    "    complete_url = web_url # initial url\n",
    "    soup = webScraper(complete_url) # get soup\n",
    "    last_page_index = getPageLimit(soup) # get last page index\n",
    "    news_headers_soup, datetime_soup = soupParser(soup) # get specific elements in a soup\n",
    "    print(\"Total pages:\", last_page_index)\n",
    "\n",
    "    '''Add data per page to dataframe'''\n",
    "    \n",
    "    # extract data\n",
    "    news_headers_per_page, datetime_per_page, contents_per_page = appendListPerPage(news_headers_soup, datetime_soup)\n",
    "    \n",
    "    # write data to df\n",
    "    df = addDFs(df, news_headers_per_page, datetime_per_page, contents_per_page)\n",
    "    \n",
    "    # from the second page to the last page\n",
    "    for i in range(3): # for demonstration 'x' is used # use 'last_page_index' in actual implementation\n",
    "        # try:\n",
    "            print(f\"- scraping {complete_url}\")\n",
    "            complete_url = getNextPageUrl(web_url, soup) # get next url\n",
    "            soup = webScraper(complete_url) # get soup\n",
    "            news_headers_soup, datetime_soup = soupParser(soup) # get specific elements in a soup\n",
    "\n",
    "            '''Add data per page to dataframe'''\n",
    "            \n",
    "            # extract data\n",
    "            news_headers_per_page, datetime_per_page, contents_per_page = appendListPerPage(news_headers_soup, datetime_soup)\n",
    "            \n",
    "            # write data to df\n",
    "            df = addDFs(df, news_headers_per_page, datetime_per_page, contents_per_page)\n",
    "        \n",
    "        # except AttributeError: # scraping the page after the last page will cause error\n",
    "            # print(\"All pages of this topic is scraped.\")\n",
    "    \n",
    "    exportExcel(df, file_name_string) # export the spreadsheet\n",
    "    # print(df) # check df of one topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da07bfd-6cd0-4030-a230-fd3e13c42a37",
   "metadata": {},
   "source": [
    "## Functions Integration to Scrape All Pages of All Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "753dc3c5-5bba-402b-9ec5-9c8df0774b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape all pages of all topics\n",
    "def scrapeAllTopics(main_url):\n",
    "    topics_soup = webScraper(main_url)\n",
    "    list_of_topics_urls = getTopicsUrls(topics_soup)\n",
    "\n",
    "    # iterate all topics\n",
    "    for topic_url in list_of_topics_urls:\n",
    "        try:\n",
    "            file_name_string = 'BBC_Burmese_topic_' + str(list_of_topics_urls.index(topic_url) + 1) # example: BBC_Burmese_topic_1\n",
    "            print(f'In the process of making \"{file_name_string}\" spreadsheet ...')\n",
    "            scrapeTopic(topic_url, file_name_string)\n",
    "            print(\"The process completes successfully.\")\n",
    "            print()\n",
    "        except:\n",
    "            print(f\"Error with the current topic url: {topic_url}\")\n",
    "            continue # continue scraping next topic\n",
    "    print(\"Hopefully, everything is scraped!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88478470-77da-439a-af73-770d676d567d",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8266704-655b-479f-88a0-2f9d083428ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Connection Error. Retrying in 5 seconds...\n",
      "  - Connection Error. Retrying in 5 seconds...\n",
      "  - Connection Error. Retrying in 5 seconds...\n",
      "In the process of making \"BBC_Burmese_topic_1\" spreadsheet ...\n",
      "Total pages: 40\n",
      "- scraping https://www.bbc.com/burmese/topics/c404v08p1wxt\n",
      "- scraping https://www.bbc.com/burmese/topics/c404v08p1wxt?page=2\n",
      "- scraping https://www.bbc.com/burmese/topics/c404v08p1wxt?page=3\n",
      "The process completes successfully.\n",
      "\n",
      "In the process of making \"BBC_Burmese_topic_2\" spreadsheet ...\n",
      "Total pages: 40\n",
      "- scraping https://www.bbc.com/burmese/topics/cnlv9j1z93wt\n",
      "- scraping https://www.bbc.com/burmese/topics/cnlv9j1z93wt?page=2\n",
      "- scraping https://www.bbc.com/burmese/topics/cnlv9j1z93wt?page=3\n",
      "The process completes successfully.\n",
      "\n",
      "In the process of making \"BBC_Burmese_topic_3\" spreadsheet ...\n",
      "Total pages: 7\n",
      "- scraping https://www.bbc.com/burmese/topics/cl3rq8rkqgxt\n",
      "- scraping https://www.bbc.com/burmese/topics/cl3rq8rkqgxt?page=2\n",
      "- scraping https://www.bbc.com/burmese/topics/cl3rq8rkqgxt?page=3\n",
      "The process completes successfully.\n",
      "\n",
      "In the process of making \"BBC_Burmese_topic_4\" spreadsheet ...\n",
      "Total pages: 2\n",
      "- scraping https://www.bbc.com/burmese/topics/cdg42x4kek0t\n",
      "- scraping https://www.bbc.com/burmese/topics/cdg42x4kek0t?page=2\n",
      "Error with the current topic url: https://www.bbc.com/burmese/topics/cdg42x4kek0t\n",
      "In the process of making \"BBC_Burmese_topic_5\" spreadsheet ...\n",
      "Total pages: 29\n",
      "- scraping https://www.bbc.com/burmese/topics/c9wpm0en9jdt\n",
      "- scraping https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=2\n",
      "- scraping https://www.bbc.com/burmese/topics/c9wpm0en9jdt?page=3\n",
      "The process completes successfully.\n",
      "\n",
      "In the process of making \"BBC_Burmese_topic_6\" spreadsheet ...\n",
      "Total pages: 40\n",
      "- scraping https://www.bbc.com/burmese/topics/cj7v92le02qt\n",
      "- scraping https://www.bbc.com/burmese/topics/cj7v92le02qt?page=2\n",
      "- scraping https://www.bbc.com/burmese/topics/cj7v92le02qt?page=3\n",
      "The process completes successfully.\n",
      "\n",
      "Hopefully, everything is scraped!\n"
     ]
    }
   ],
   "source": [
    "scrapeAllTopics(main_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b22bc72-57e2-4dfe-b80d-1100bceb51d6",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "- The while loop is running indefinitely until the connection comes back. There may be a better solution.\n",
    "- It would take a lengthy time to scrape all pages of all topics. There should be ways to fasten the process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
